# Code Generation Readiness: Design Gaps & Risks

This document outlines the critical gaps between the current parser implementation and the requirements for a robust C code generator/runtime for OrgLang.

## 1. The Runtime Implementation Gap

We have extensive *designs* (see `legacy/` sketches for Arena/Scheduler and `docs/number_support.md` for GMP), but currently **zero implementation code** for the runtime system.

* **Risk**: We cannot simply "emit C code" without a target runtime library.
* **Requirement**: A "Kernel" (`liborg.a` or similar) that provides:
  * **Memory Management**: The `Arena` allocator.
  * **Value System**: The `OrgValue` tagged union struct (integrating GMP types).
  * **Scheduler**: The M:N hybrid scheduler implementation.
* **Recommendation**: Treat "Implementing the Runtime Kernel" as a distinct phase before or parallel to the Code Generator.

## 2. GMP & Memory Management Complexity

The plan to use **GMP** (`mpz_t`, `mpq_t`) for arbitrary precision is sound, but integrating it with a custom **Arena allocator** is non-trivial.

* **Complexity**: We must use `mp_set_memory_functions` to redirect GMP's internal `malloc/free` calls to our Arena. Misconfiguration risks massive memory leaks or segmentation faults, especially with `mpq_t` (Rationals) which perform frequent internal allocations.
* **Decimal Logic**: The custom `Decimal` type (`mpq_t` + `long scale`) described in `number_support.md` requires custom arithmetic logic (manual scale management) implemented in C helper functions (`org_decimal_add`, etc.), not just inline emission.

## 3. Dynamic Dispatch vs. Static C

OrgLang is dynamically typed, but we are targeting C (statically typed).

* **Challenge**: Operations like `a + b` are polymorphic. They must handle Integers, Decimals, Rationals, Tables (size), or Strings (length/concatenation depending on semantics).
* **Implication**: The code generator cannot emit simple C operators (`a + b`). It must emit calls to a runtime dispatcher (e.g., `org_add(a, b)`).
* **Performance**: This introduces runtime overhead for type checking and dispatch, which the runtime implementation must minimize (e.g., via tagged pointer optimizations).

## 4. Lazy Evaluation Semantics

`README.md` strictly promises **Lazy Evaluation** ("Values within a table are lazy by default").

* **Codegen Impact**: We cannot evaluate expressions immediately upon parsing. The code generator must wrap expressions in **Thunks** (closures) that can be passed around and evaluated only when demanded (e.g., by `.` access or `?` operator).
* **Risk**: C lacks native closures. We need a mechanism to:
  * Capture environments (stack frames).
  * Store them in heap-allocated (Arena-allocated) objects.
  * Invoke them later with the correct context.

## 5. Resources & Side Effects

Resources like `@stdout` are parsed, but their C implementation is undefined.

* **Missing Link**: We need a C API for resources (`org_resource_open`, `org_resource_write`, `org_resource_close`) that:
  * Abstracts over file descriptors/sockets.
  * Integrates with the **Scheduler** to preventing blocking I/O on scheduler threads (using `epoll`/`kqueue` or non-blocking I/O).

## 6. Runtime Implementation Plan

An actionable plan to bridge the gaps identified above.

### 6.1 Phase 1: The Arena Kernel

The foundation of the runtime is the **Arena Allocator**, but integrating it with GMP (`mpz_t`, `mpq_t`) requires solving specific architectural challenges.

#### 6.1.1 The Global Hook Problem

GMP's `mp_set_memory_functions` sets **global** function pointers for allocation. However, in our M:N scheduler, multiple Fibers run concurrently (on different threads), each with its own isolated Arena.

* **Difficuly**: `org_gmp_alloc` is called by GMP without passing any context (it only takes `size_t`).
* **Solution**: Use **Thread-Local Storage (TLS)**.
  * Define `__thread OrgArena* current_fiber_arena;`.
  * The **Scheduler** is responsible for updating `current_fiber_arena` every time it swaps a Fiber onto an OS thread.
  * The `org_gmp_alloc` wrapper reads this TLS variable to know *where* to allocate.

#### 6.1.2 The "Realloc" & "Free" Semantics

Implementing `realloc` and `free` for a linear bump-pointer arena has obscure edge cases:

* **`org_gmp_free` is a No-Op**:
  * Since we cannot reclaim space in the middle of a bump-ptr arena, `free` does nothing.
  * **Consequence**: Temporary values generated by GMP (e.g. intermediate results in `a * b + c`) consume arena space that is **not reclaimed** until the entire Fiber/Pulse finishes. This puts pressure on the Scheduler to ensure high-granularity tasks.

* **`org_gmp_realloc` Complexity**:
  * GMP frequently resizes buffers. In `malloc` world, this often happens in-place.
  * In Arena world, we can only resize in-place if the object is the **very last allocation**.
  * **Strategy**:
        1. Check if `ptr + old_size == arena->current_ptr`.
        2. **If yes**: Just bump `arena->current_ptr` by `(new_size - old_size)` (checking capacity).
        3. **If no**: Allocate a completely new block of `new_size`, `memcpy` the data, and leave the old block as "wasted" space (to be reclaimed at bulk reset).

### 6.2 Resource Lifecycle: Block vs. Expression

We analyzed the trade-offs between scoping resources to **Blocks** `{ ... }` versus **Expressions**.

| Model | Semantics | Pros | Cons |
| :--- | :--- | :--- | :--- |
| **Block-Scoped** | Resources created at `{`, destroyed at `}` | Deterministic; Ties perfectly to Arena "stack frames"; Simple compiler logic. | Higher memory usage (resources live longer than needed). |
| **Expression-Scoped** | `expr -> res -> expr` | Minimal memory footprint; Resources exist only during data flow. | Complex to define boundaries; Requires passing ephemeral Arena handles; "Invisible" scopes. |

**Decision**: We will proceed with **Block-Scoped** resources for the initial implementation.

* It maps 1:1 with the Arena design (one Arena per Fiber/Block).
* Optimization (expression-scoped) can be added later as a compiler analysis pass that inserts early `destroy` calls.

### 6.3 Scheduler Optimization: Map vs. Task

The scheduler must efficiently handle the mix of pure computation and side effects.

* **Current Operator (`->`) Logic**: Recursively evaluates.
* **New Logic**: The runtime's `org_op_arrow` function will inspect the Right-Hand Side (RHS):
    1. **Pure Function (Map)**: If RHS is a standard Function/Operator, execute it **synchronously** in the current Fiber. This optimizes cases like `[1 2 3] -> add1` into a tight loop without scheduler overhead.
    2. **Resource/Effect (Task)**: If RHS is a Resource Instance, treat it as an I/O Task.
        * Submit the work to the Scheduler.
        * **Yield** the current Fiber.
        * Resume when the Resource signals completion (via `next` return value or callback).

### 6.4 Dynamic Dispatch: Tagged Pointers

To alleviate the overhead of dynamic typing and GMP allocations for simple operations (e.g., loop counters), we will use **Tagged Pointer Optimization**.

* **Concept**: Since `Arena` allocations are 8-byte aligned, the lower 3 bits of a pointer are always `000`. We use these bits to store a type tag.
* **Representation**: `OrgValue` is a `uint64_t` (opaque handle).
  * `.......00` -> **Pointer**: Valid pointer to heap object (BigInt, Table, String).
  * `.......01` -> **Small Integer**: 63-bit signed integer (immediate).
  * `.......10` -> **Special**: Boolean (`...0110`=True, `...0010`=False), Null, Error.
  * `.......11` -> **Reserved**.

* **Fast Path Dispatch**:
  * The dispatcher (`org_add`) first checks `(a & 1) && (b & 1)`.
  * **If true**: Both are Small Integers. Perform CPU-native addition. Check for overflow.
    * **No Overflow**: Return tagged result.
    * **Overflow**: Promote to GMP BigInt (allocate in Arena) and return Pointer.
  * **If false**: Fall back to full `mpz_t` arithmetic (promoting SmallInts to BigInts if needed).

### 6.5 Implementation Roadmap

1. **`pkg/runtime/core`**: Implement `arena.c`, `values.c` (OrgValue tagged union).
2. **`pkg/runtime/gmp`**: Implement GMP glue code.
3. **`pkg/runtime/sched`**: Implement `fiber.c` and a single-threaded M:N scheduler loop.
